,paper_url,paper_title,paper_description,publication_date,tags,excluded_tags,keyword_tags,should_be_included,,,,
0,/citations?view_op=view_citation&hl=en&user=09enRf4AAAAJ&sortby=pubdate&citation_for_view=09enRf4AAAAJ:hqOjcs7Dif8C,determining the secondary structure of elapid toxins using multi-layer perceptrons and kohonen networks,,2018/4,['ai_keywords'],[],['multi layer perceptron'],Yes,,Yes,No,Unsure
1,/citations?view_op=view_citation&hl=en&user=09enRf4AAAAJ&sortby=pubdate&citation_for_view=09enRf4AAAAJ:zYLM7Y9cAGgC,classifying snake toxins using decision trees,"Snake toxins are a group of peptides found in snake venoms, which have some form of adverse physiological effects on humans and/or animals. Primarily these consist of neurotoxins, cytotoxins and cardiotoxins, which directly affect the nervous system, skin and muscle tissues, and the heart muscle tissues respectively. In this short paper, we show how, given a protein sequence, we can classify it either as a neurotoxic, a cytotoxic or a cardiotoxic peptide using decision trees. Such classification can be helpful when the phsiological effects of a new venom peptide need to be determined given its sequence information only.",2016/8,['ai_keywords'],[],['decision tree'],Yes,,,,
2,/citations?view_op=view_citation&hl=en&user=0E4dHV8AAAAJ&sortby=pubdate&citation_for_view=0E4dHV8AAAAJ:u-x6o8ySG0sC,oil price shocks and european industries,We investigate the impact of oil price shocks at the industry level in the Euro area for the period 1983–2007. We use different oil price specifications and use dynamic VAR models and multivariate regression to investigate how 38 different industries respond to oil price shocks. We pay specific attention to the asymmetry of the industries' responses regarding oil price increases and decreases. We find that the impact of oil price shocks substantially differs along the different industries. We find that the significance of this result also differs along the various oil price specifications. The results are quite robust to the way in which we model the problem.,2012/7/1,['ai_keywords'],[],['regression'],No,,,,
3,/citations?view_op=view_citation&hl=en&user=36oPDJcAAAAJ&sortby=pubdate&citation_for_view=36oPDJcAAAAJ:u5HHmVD_uO8C,assessing risk of stealing proprietary models for medical imaging tasks,"The success of deep learning in medical imaging applications has led several companies to deploy proprietary models in diagnostic workflows, offering monetized services. Even though model weights are hidden to protect the intellectual property of the service provider, these models are exposed to model stealing (MS) attacks, where adversaries can clone the model’s functionality by querying it with a proxy dataset and training a thief model on the acquired predictions. While extensively studied on general vision tasks, the susceptibility of medical imaging models to MS attacks remains inadequately explored. This paper investigates the vulnerability of black-box medical imaging models to MS attacks under realistic conditions where the adversary lacks access to the victim model’s training data and operates with limited query budgets. We demonstrate that adversaries can effectively execute MS attacks by using …",2024/10/3,['ai_keywords'],[],['deep learning'],,,,,
4,/citations?view_op=view_citation&hl=en&user=4dfRX6oAAAAJ&sortby=pubdate&citation_for_view=4dfRX6oAAAAJ:u5HHmVD_uO8C,federated sinkhorn,"In this work we investigate the potential of solving the discrete Optimal Transport (OT) problem with entropy regularization in a federated learning setting. Recall that the celebrated Sinkhorn algorithm transforms the classical OT linear program into strongly convex constrained optimization, facilitating first order methods for otherwise intractably large problems. A common contemporary setting that remains an open problem as far as the application of Sinkhorn is the presence of data spread across clients with distributed inter-communication, either due to clients whose privacy is a concern, or simply by necessity of processing and memory hardware limitations. In this work we investigate various natural procedures, which we refer to as Federated Sinkhorn, that handle distributed environments where data is partitioned across multiple clients. We formulate the problem as minimizing the transport cost with an entropy regularization term, subject to marginal constraints, where block components of the source and target distribution vectors are locally known to clients corresponding to each block. We consider both synchronous and asynchronous variants as well as all-to-all and server-client communication topology protocols. Each procedure allows clients to compute local operations on their data partition while periodically exchanging information with others. We provide theoretical guarantees on convergence for the different variants under different possible conditions. We empirically demonstrate the algorithms performance on synthetic datasets and a real-world financial risk assessment application. The investigation highlights the subtle tradeoffs …",2025/2/10,['ai_keywords'],[],"['synthetic data', 'federated learning']",Yes,,,,
5,/citations?view_op=view_citation&hl=en&user=5Q3R8rsAAAAJ&sortby=pubdate&citation_for_view=5Q3R8rsAAAAJ:2osOgNQ5qMEC,implications of being discrete and spatial for detecting early warning signals of regime shifts,"Theory suggests that ecological systems exhibit a pronounced slow down in their dynamics, known as ‘critical slowing down’ (CSD), before they undergo regime shifts or critical transitions. As a result of CSD, ecosystems exhibit characteristic temporal and spatial changes which can be used as early warning signals of imminent regime shifts. For temporal data, statistical methods to detect these generic indicators of ecosystem resilience are well developed. However, for spatial data, despite a well developed theoretical framework, statistical methods such as data pre-processing and null models to detect EWS are relatively poorly developed. In this manuscript, we investigate the case of a common type of ecological spatial dataset which consists of binary values at each location (e.g. occupied/unoccupied, tree/grass or coralline/bleached). We employ a cellular-automaton based spatially-explicit model which …",2018/11/1,['ai_keywords'],[],['data pre processing'],No,,,,
6,/citations?view_op=view_citation&hl=en&user=6UgVkhUAAAAJ&sortby=pubdate&citation_for_view=6UgVkhUAAAAJ:9yKSN-GCB0IC,a global data-driven forecasting approach for buildings energy demand prediction,"Energy demand prediction is a key factor for buildings operation optimization, and energy conservations. Recently, the rapid advancement of sensing technology, and smart meters in the buildings sector has allowed to record and collect large amount of building energy datasets which can provides the opportunity to understand how these buildings are being used to optimize and reduce their daily energy usage. Nevertheless, the majority of existing works have mostly been focused on the utilization of data-driven approaches to predict the future energy demand for one building at once. This study attempts to address this gap by proposing a Global Data-driven Forecasting Approach which has been trained with several time series datasets from a group of residential buildings. Utilizing the proposed approach offers numerous benefits including: (I) better generalisation ability on predicting the future energy demand for …",2023/7/7,['ai_keywords'],[],['data driven'],Yes,,,,
7,/citations?view_op=view_citation&hl=en&user=6UgVkhUAAAAJ&sortby=pubdate&citation_for_view=6UgVkhUAAAAJ:Tyk-4Ss8FVUC,predicting the impact of climate change on building energy consumption by using data-driven approaches,"Buildings are complex thermodynamic entities that account for a large proportion of energy consumption. This work explores the application of data-driven models in order to forecast the building energy consumption over long time horizons. Long Short-Term-Memory and Random Forest are used to forecast hourly heating and cooling energy consumption in the Urban Sciences Building, that is located in the city of Newcastle upon Tyne, United Kingdom. A synthetic time-series dataset is constructed using: (I) a validated EnergyPlus model, and (II) operational data hosted by Newcastle Urban Observatory. The energy consumption forecast is made using future climate data that represents a high emission future scenario during a typical year, i.e., 2030, and 2080. The experimental results suggest that, on average over the next six decades, there will be a 45% reduction in annual heating load, accompanied by a 680 …",2024/12/24,['ai_keywords'],[],"['data driven', 'random forest', 'long short term memory']",Yes,,,,
8,/citations?view_op=view_citation&hl=en&user=6UgVkhUAAAAJ&sortby=pubdate&citation_for_view=6UgVkhUAAAAJ:Tyk-4Ss8FVUC,predicting the impact of climate change on building energy consumption by using data-driven approaches,"Buildings are complex thermodynamic entities that account for a large proportion of energy consumption. This work explores the application of data-driven models in order to forecast the building energy consumption over long time horizons. Long Short-Term-Memory and Random Forest are used to forecast hourly heating and cooling energy consumption in the Urban Sciences Building, that is located in the city of Newcastle upon Tyne, United Kingdom. A synthetic time-series dataset is constructed using: (I) a validated EnergyPlus model, and (II) operational data hosted by Newcastle Urban Observatory. The energy consumption forecast is made using future climate data that represents a high emission future scenario during a typical year, i.e., 2030, and 2080. The experimental results suggest that, on average over the next six decades, there will be a 45% reduction in annual heating load, accompanied by a 680 …",2024/12/24,['ai_keywords'],[],"['data driven', 'random forest', 'long short term memory']",Yes,,,,
9,/citations?view_op=view_citation&hl=en&user=9go62_cAAAAJ&sortby=pubdate&citation_for_view=9go62_cAAAAJ:qjMakFHDy7sC,price jump detection in limit order book,"A limit order book provides information on available limit order prices and their volumes. Based on these quantities, we give an empirical result on the relationship between the bid-ask liquidity balance and trade sign and we show that liquidity balance on best bid/best ask is quite informative for predicting the future market order's direction. Moreover, we de ne price jump as a sell (buy) market order arrival which is executed at a price which is smaller (larger) than the best bid (best ask) price at the moment just after the precedent market order arrival. Features are then extracted related to limit order volumes, limit order price gaps, market order information and limit order event information. Logistic regression is applied to predict the price jump from the limit order book's feature. LASSO logistic regression is introduced to help us make variable selection from which we are capable to highlight the importance of di erent features in predicting the future price jump. In order to get rid of the intraday data seasonality, the analysis is based on two separated datasets: morning dataset and afternoon dataset. Based on an analysis on forty largest French stocks of CAC40, we nd that trade sign and market order size as well as the liquidity on the best bid (best ask) are consistently informative for predicting the incoming price jump.",2012/3/14,['ai_keywords'],[],['regression'],Unsure,,,,
10,/citations?view_op=view_citation&hl=en&user=9go62_cAAAAJ&sortby=pubdate&citation_for_view=9go62_cAAAAJ:u-x6o8ySG0sC,price jump prediction in limit order book,"A limit order book provides information on available limit order prices and their volumes. Based on these quantities, we give an empirical result on the relationship between the bid-ask liquidity balance and trade sign and we show that liquidity balance on best bid/best ask is quite informative for predicting the future market order's direction. Moreover, we define price jump as a sell (buy) market order arrival which is executed at a price which is smaller (larger) than the best bid (best ask) price at the moment just after the precedent market order arrival. Features are then extracted related to limit order volumes, limit order price gaps, market order information and limit order event information. Logistic regression is applied to predict the price jump from the limit order book's feature. LASSO logistic regression is introduced to help us make variable selection from which we are capable to highlight the importance of different features in predicting the future price jump. In order to get rid of the intraday data seasonality, the analysis is based on two separated datasets: morning dataset and afternoon dataset. Based on an analysis on forty largest French stocks of CAC40, we find that trade sign and market order size as well as the liquidity on the best bid (best ask) are consistently informative for predicting the incoming price jump.",2012/4/6,['ai_keywords'],[],['regression'],No,,,,
11,/citations?view_op=view_citation&hl=en&user=Dn0Ext4AAAAJ&sortby=pubdate&citation_for_view=Dn0Ext4AAAAJ:u-x6o8ySG0sC,a new computationally efficient mamdani interval type-2 fuzzy modelling framework,"A modified center-of-sums (mCoS) type-reduction technique is proposed in this paper for constructing a data-driven Mamdani interval type-2 fuzzy modelling (MIT2FM) framework. The mCoS type-reducer is an extension of its type-1 counterpart, the center-of-sums defuzzification, which takes both the area of the scaled consequent membership function of each fired rule and its associated geometric center into account for computing the final output. Contrary to the normal center-of-sums type-reduction, the proposed approach considers the full area under the scaled consequent membership functions even if such area extends beyond the range of the output variable. This enables the commonly used Gaussian interval type-2 membership functions to be utilised in MIT2FM, the area of which has to be calculated via the improper integrals over the whole real line. Moreover, the mCoS method can make use of the mean …",2012/6/10,['ai_keywords'],[],['data driven'],Unsure,,,,
12,/citations?view_op=view_citation&hl=en&user=F2lMPBwAAAAJ&sortby=pubdate&citation_for_view=F2lMPBwAAAAJ:-f6ydRqryjwC,"digital twin technology in the rail industry: adutch qualitative case study on success factors, challenges, and future use cases","The term Digital Twin (DT) emerged in the early 2010s, but its concepts have roots dating back to the 1960s at NASA (Rajamurugu & Karthik, 2022). A DT is a virtual representation of an object or system throughout its lifecycle (Singh et al., 2021), leveraging real-time data, simulation, machine learning, and reasoning to inform decision-making (Goodwin et al., 2024). It can also influence the physical world based on changes in the virtual model. While there is no consensus on a definition of a DT, most research views it as a cyber-physical system that shares the concepts of a physical entity, a virtual model, and connections between both (Liu et al., 2022). This study aligns with the definition of a DT as"" a set of linked operation data artifacts and (simulation) models, which are of suitable granularity for their intended purpose and stakeholders and evolve throughout the product life-cycle""(Boschert & Rosen, 2016).",2024/5,['ai_keywords'],[],['machine learning'],Yes,,,,
13,/citations?view_op=view_citation&hl=en&user=F2lMPBwAAAAJ&sortby=pubdate&citation_for_view=F2lMPBwAAAAJ:hFOr9nPyWt4C,decision mining in the rail industry: a,"Top-performing organizations typically employ agile decision-making based on rigorous analysis and use these insights to improve their day-to-day operations as well as to guide future strategies (LaValle et al., 2010). However, the upfront understanding of organizational decision-making is paramount for successful business analytics implementations (Sharma et al., 2014). Therefore, these do not inherently create value, especially since the technologies should merely be seen as tools—not drivers—that aid in dealing with information overload (Edmunds & Morris, 2000). The rapid advances in information technology have led to the paradoxical condition that, even though available information is abundant, it is more difficult to extract relevant and useful information when needed (Edmunds & Morris, 2000). Nevertheless, the potential value of improved decision-making enabled by inclusion of contextual process information justifies investments in new forms of data-driven analytics (Sharma et al., 2014).A promising research area in data-driven analytics is process mining (van der Aalst & Weijters, 2004. Process mining allows not only for the investigation of causal relations between activities but also additional data attributes that enable the investigation of performance (timestamps) and workload (resources)(van der Aalst & Weijters, 2004). With the abundance of data available, it becomes increasingly relevant to critically assess and evaluate event log quality (Kherbouche et al., 2016). While research has been carried out to address these latter aspects for event logs (Fischer et al., 2020; Suriadi et al., 2017; van Wensveen, 2020), limited attempts …",2024/5,['ai_keywords'],[],['data driven'],No,,,,
14,/citations?view_op=view_citation&hl=en&user=GXTFeGEAAAAJ&sortby=pubdate&citation_for_view=GXTFeGEAAAAJ:9yKSN-GCB0IC,monthly rainfall trends in greece (1950-2012),"Trends in monthly rainfall during the period from 1950 to 2012 and the variability thereof in space and time are investigated. The time series analysed are from 120 stations and cover mainly the continental part of Greece. To estimate their trends, linear regression is used for each time series separately for:(a) the entire record length (1950-2012) and (b) for each half of the period (1950-1981 and 1982-2012). A spatially aggregated time series of rainfall over Greece is also produced and its correlations with climatic features of the northern hemisphere are explored.",2014/5,['ai_keywords'],[],['regression'],No,,,,
15,/citations?view_op=view_citation&hl=en&user=HpfnZ9AAAAAJ&sortby=pubdate&citation_for_view=HpfnZ9AAAAAJ:2osOgNQ5qMEC,"automatyczne znakowanie sensami słów [eng.: automatic wsd annotation], chapter in narodowy korpus języka polskiego [eng.: national corpus of polish]",,2012,['ai_keywords'],[],[' corpus '],Unsure,,,,
16,/citations?view_op=view_citation&hl=en&user=HpfnZ9AAAAAJ&sortby=pubdate&citation_for_view=HpfnZ9AAAAAJ:3fE2CSJIrl8C,detection of nested mentions for coreference resolution in polish," This paper describes the results of creating a shallow grammar of Polish capable of detecting multi-level nested nominal phrases, intended to be used as mentions in coreference resolution tasks. The work is based on existing grammar developed for the National Corpus of Polish and evaluated on manually annotated Polish Coreference Corpus.",2014,['ai_keywords'],[],[' corpus '],No,,,,
17,/citations?view_op=view_citation&hl=en&user=HpfnZ9AAAAAJ&sortby=pubdate&citation_for_view=HpfnZ9AAAAAJ:8k81kl-MbHgC,deliverable d5. text summarisation tools,"D5. 1 is the main deliverable of the WP5 “Summarisation” in the ATLAS project. It describes the text summarization tools for Bulgarian, German, Greek, English, Polish,",2012,['ai_keywords'],[],"['text summarisation', 'text summarization']",Unsure,,,,
18,/citations?view_op=view_citation&hl=en&user=HpfnZ9AAAAAJ&sortby=pubdate&citation_for_view=HpfnZ9AAAAAJ:MXK_kJrjxJIC,polish coreference corpus,,2013,['ai_keywords'],[],[' corpus '],No,,,,
19,/citations?view_op=view_citation&hl=en&user=HpfnZ9AAAAAJ&sortby=pubdate&citation_for_view=HpfnZ9AAAAAJ:Tyk-4Ss8FVUC,interesting linguistic features in coreference annotation of an inflectional language," This paper reports on linguistic features and decisions that we find vital in the process of annotation and resolution of coreference for highly inflectional languages. The presented results have been collected during preparation of a corpus of general direct nominal coreference of Polish. Starting from the notion of a mention, its borders and potential vs. actual referentiality, we discuss the problem of complete and near-identity, zero subjects and dominant expressions. We also present interesting linguistic cases influencing the coreference resolution such as the difference between semantic and syntactic heads or the phenomenon of coreference chains made of indefinite pronouns.",2013/10/10,['ai_keywords'],[],[' corpus '],Unsure,,,,
20,/citations?view_op=view_citation&hl=en&user=HpfnZ9AAAAAJ&sortby=pubdate&citation_for_view=HpfnZ9AAAAAJ:Zph67rFs4hoC,mention detection for coreference resolution in polish. development of the formal grammar,"This paper presents the results of an improvement and extension of the Shallow Grammar of Polish, designed for the needs of the Computer-based Methods for Coreference Resolution in Polish Texts (CORE) project. The role of the Grammar was to detect nominal groups (i.e. multi-level nested phrases) that could be considered as mentions in coreference resolution tasks. In this article, the reorganization and changes to the Grammar are described, as well as the results of an evaluation of the Polish Coreference Corpus with manual annotations of mentions and coreferential expressions. A comparison of the second version of the Grammar with an evaluation of the first version reveals an improvement to the recall and F1 measures.",2016/12/31,['ai_keywords'],[],[' corpus '],No,,,,
21,/citations?view_op=view_citation&hl=en&user=HpfnZ9AAAAAJ&sortby=pubdate&citation_for_view=HpfnZ9AAAAAJ:_kc_bZDykSQC,polish coreference corpus," The Polish Coreference Corpus (PCC) is a large corpus of Polish general nominal coreference built upon the National Corpus of Polish. With its 1900 documents from 14 text genres, containing about 540,000 tokens, 180,000 mentions and 128,000 coreference clusters, the PCC is among the largest coreference corpora in the international community. It has some novel features, such as the annotation of the quasi-identity relation, inspired by Recasens’ near-identity, as well as the mark-up of semantic heads and dominant expressions. It shows a good inter-annotator agreement and is distributed in three formats under an open license. Its by-products include freely available annotation tools with custom features such as file distribution management and annotation adjudication.",2016,['ai_keywords'],[],[' corpus '],,,,,
22,/citations?view_op=view_citation&hl=en&user=HpfnZ9AAAAAJ&sortby=pubdate&citation_for_view=HpfnZ9AAAAAJ:aqlVkmm33-oC,three-step coreference-based summarizer for polish news texts,"This article addresses the problem of automatic summarization of press articles in Polish. The main novelty of this research lays in the proposal of a three-step summarization algorithm which benefits from using coreference information.In related work section, all coreference-based approaches to summarization are presented. Then we describe in detail all publicly available summarization tools developed for Polish language. We state the problem of single-document press article summarization for Polish, describing the training and evaluation dataset: the POLISH SUMMARIES CORPUS.Next, a new coreference-based extractive summarization system NICOLAS is introduced. Its algorithm utilises advanced third-party preprocessing tools to extract the coreference information from the text to be summarized. This information is transformed into a complex set of features related to coreference concepts (mentions and …",2019/6/26,['ai_keywords'],[],[' corpus '],,,,,
23,/citations?view_op=view_citation&hl=en&user=HpfnZ9AAAAAJ&sortby=pubdate&citation_for_view=HpfnZ9AAAAAJ:d1gkVwhDpl0C,word sense disambiguation in the national corpus of polish,,2012,['ai_keywords'],[],[' corpus '],,,,,
24,/citations?view_op=view_citation&hl=en&user=HpfnZ9AAAAAJ&sortby=pubdate&citation_for_view=HpfnZ9AAAAAJ:u5HHmVD_uO8C,semantic similarity functions in word sense disambiguation," This paper presents a method of improving the results of automatic Word Sense Disambiguation by generalizing nouns appearing in a disambiguated context to concepts. A corpus-based semantic similarity function is used for that purpose, by substituting appearances of particular nouns with a set of the most closely related similar words. We show that this approach may be applied to both supervised and unsupervised WSD methods and in both cases leads to an improvement in disambiguation accuracy. We evaluate the proposed approach by conducting a series of lexical sample WSD experiments on both domain-restricted dataset and a general, balanced Polish-language text corpus.",2012,['ai_keywords'],[],[' corpus '],,,,,
25,/citations?view_op=view_citation&hl=en&user=IBCSEJYAAAAJ&sortby=pubdate&citation_for_view=IBCSEJYAAAAJ:u-x6o8ySG0sC,a broad spectrum chemokine inhibitor prevents preterm labor but not microbial invasion of the amniotic cavity or neonatal morbidity in a non-human primate model,"Leukocyte activation within the chorioamniotic membranes is strongly associated with inflammation and preterm labor (PTL). We hypothesized that prophylaxis with a broad-spectrum chemokine inhibitor (BSCI) would downregulate the inflammatory microenvironment induced by Group B Streptococcus (GBS, Streptococcus agalactiae) to suppress PTL and microbial invasion of the amniotic cavity (MIAC). To correlate BSCI administration with PTL and MIAC, we used a unique chronically catheterized non-human primate model of Group B Streptococcus (GBS)-induced PTL. In the early third trimester (128–138 days gestation; ~29–32 weeks human pregnancy), animals received choriodecidual inoculations of either: (1) saline (N = 6), (2) GBS, 1–5 × 108 colony forming units (CFU)/ml; N = 5), or (3) pre-treatment and daily infusions of a BSCI (10 mg/kg intravenous and intra-amniotic) with GBS (1–5 × 108 CFU/ml; N = 4). We measured amniotic cavity pressure (uterine contraction strength) and sampled amniotic fluid (AF) and maternal blood serially and cord blood at delivery. Cesarean section was performed 3 days post-inoculation or earlier for PTL. Data analysis used Fisher's exact test, Wilcoxon rank sum and one-way ANOVA with Bonferroni correction. Saline inoculation did not induce PTL or infectious sequelae. In contrast, GBS inoculation typically induced PTL (4/5, 80%), MIAC and fetal bacteremia (3/5; 60%). Remarkably, PTL did not occur in the BSCI+GBS group (0/4, 0%; p = 0.02 vs. GBS), despite MIAC and fetal bacteremia in all cases (4/4; 100%). Compared to the GBS group, BSCI prophylaxis was associated with significantly lower cytokine …",2020/4/30,['ai_keywords'],[],[' ml '],,,,,
26,/citations?view_op=view_citation&hl=en&user=NcuBBGAAAAAJ&sortby=pubdate&citation_for_view=NcuBBGAAAAAJ:u-x6o8ySG0sC,face recognition using local binary patterns histograms (lbph) on an fpga-based system on chip (soc),"The need for facial recognition systems that are fast and accurate is continuously increasing. In this paper, a face recognition implementation on a System on Chip (SoC), integrated with an FPGA, is presented. This implementation utilizes Local Binary Patterns Histograms to extract features from test face images and Manhattan distance to retrieve the correct match from the system's face database. The SoC utilized is a Zynq-7030. The feature extraction and the distance computations, between the database, are implemented on the FPGA. The ARM processor of the SoC is responsible for receiving the input stream and presenting the output result, using the acquired distances. Real-time face recognition, with an execution time of 8.6 ms and accuracy of 79%, is achieved through this implementation.",2016/5/23,['ai_keywords'],[],['facial recognition'],,,,,
27,/citations?view_op=view_citation&hl=en&user=NcuBBGAAAAAJ&sortby=pubdate&citation_for_view=NcuBBGAAAAAJ:u-x6o8ySG0sC,face recognition using local binary patterns histograms (lbph) on an fpga-based system on chip (soc),"The need for facial recognition systems that are fast and accurate is continuously increasing. In this paper, a face recognition implementation on a System on Chip (SoC), integrated with an FPGA, is presented. This implementation utilizes Local Binary Patterns Histograms to extract features from test face images and Manhattan distance to retrieve the correct match from the system's face database. The SoC utilized is a Zynq-7030. The feature extraction and the distance computations, between the database, are implemented on the FPGA. The ARM processor of the SoC is responsible for receiving the input stream and presenting the output result, using the acquired distances. Real-time face recognition, with an execution time of 8.6 ms and accuracy of 79%, is achieved through this implementation.",2016/5/23,['ai_keywords'],[],['facial recognition'],,,,,
28,/citations?view_op=view_citation&hl=en&user=RSKJehcAAAAJ&sortby=pubdate&citation_for_view=RSKJehcAAAAJ:1sJd4Hv_s6UC,search for the decay,"We present the first experimental search for the rare charm decay . It is based on an  collision sample consisting of  pairs of  mesons collected by the BESIII detector at , corresponding to an integrated luminosity of . A data-driven method is used to ensure the reliability of the background modeling. No significant  signal is observed in data and an upper limit of the branching fraction is set to be  at the 90% confidence level. This is the first experimental constraint on charmed-hadron decays into dineutrino final states.",2022/4/1,['ai_keywords'],[],['data driven'],,,,,
29,/citations?view_op=view_citation&hl=en&user=RSKJehcAAAAJ&sortby=pubdate&citation_for_view=RSKJehcAAAAJ:NMxIlDl6LWMC,detection of the diffuse supernova neutrino background with juno,"As an underground multi-purpose neutrino detector with 20 kton liquid scintillator, Jiangmen Underground Neutrino Observatory (JUNO) is competitive with and complementary to the water-Cherenkov detectors on the search for the diffuse supernova neutrino background (DSNB). Typical supernova models predict 2-4 events per year within the optimal observation window in the JUNO detector. The dominant background is from the neutral-current (NC) interaction of atmospheric neutrinos with 12C nuclei, which surpasses the DSNB by more than one order of magnitude. We evaluated the systematic uncertainty of NC background from the spread of a variety of data-driven models and further developed a method to determine NC background within 15\% with {\it {in}}{\it {situ}} measurements after ten years of running. Besides, the NC-like backgrounds can be effectively suppressed by the intrinsic pulse-shape discrimination (PSD) capabilities of liquid scintillators. In this talk, I will present in detail the improvements on NC background uncertainty evaluation, PSD discriminator development, and finally, the potential of DSNB sensitivity in JUNO.",2022,['ai_keywords'],[],['data driven'],,,,,
30,/citations?view_op=view_citation&hl=en&user=YhogBoEAAAAJ&sortby=pubdate&citation_for_view=YhogBoEAAAAJ:qjMakFHDy7sC,non-local tuning of quantum current fluctuations in multi-connected lattices with and without external reservoirs,"Fluctuations arising from non-commutative operators in Heisenberg's uncertainty relation are fundamental in quantum mechanics. The current operator J i, i+ 1 measures current between two lattice sites and does not commute with the interconnected current operator J i+ 1, i+ 2. This means the current fluctuations are correlated and proportional to J i, i+ 2. Quantum fluctuations of current are affected by the tunneling strength between the conjoining next-nearest-neighbor lattice sites. Open system and closed system methods are used to determine how the tunneling coefficient affects fluctuations for non-interacting and interacting fermions. Investigations illustrate a pure quantum current effect since the wave nature of quantum particles leads to non-local correlations.",2018,['ai_keywords'],[],['nearest neighbor'],,,,,
31,/citations?view_op=view_citation&hl=en&user=YujN82YAAAAJ&sortby=pubdate&citation_for_view=YujN82YAAAAJ:kNdYIx-mwKoC,a robust quantum memory for quantum networks,"Quantum networks are a promising technology with applications in quantum computation and communication. The key idea is to use photons to entangle network nodes, which contain data qubits that can robustly store and process quantum states. The size and complexity of current quantum network protocols are limited by the decoherence of the data qubits during the entanglement-link generation process [1, 2]. In this work, we introduce a new type of data qubit for quantum networks, consisting of a nearest-neighbours pair of C13 nuclear spins [3] in a diamond lattice. We show that this qubit provides an extremely coherent quantum memory, that is also robust to the optical entanglement links. This extended coherence is expected to enable a new generation of quantum network protocols.",2024/3/5,['ai_keywords'],[],['nearest neighbour'],,,,,
32,/citations?view_op=view_citation&hl=en&user=c6bK3vUAAAAJ&sortby=pubdate&citation_for_view=c6bK3vUAAAAJ:35N4QoGY0k4C,strategies for improving the distribution of random function outputs in gsgp," In the last years, different approaches have been proposed to introduce semantic information to genetic programming. In particular, the geometric semantic genetic programming (GSGP) and the interesting properties of its evolutionary operators have gotten the attention of the community. This paper is interested in the use of GSGP to solve symbolic regression problems, where semantics is defined by the output set generated by a given individual when applied to the training cases. In this scenario, both mutation and crossover operators defined with fitness function based on Manhattan distance use randomly built functions to generate offspring. However, the outputs of these random functions are not guaranteed to be uniformly distributed in the semantic space, as the functions are generated considering the syntactic space. We hypothesize that the non-uniformity of the semantics of these functions may bias …",2017,['ai_keywords'],[],['regression'],,,,,
33,/citations?view_op=view_citation&hl=en&user=c6bK3vUAAAAJ&sortby=pubdate&citation_for_view=c6bK3vUAAAAJ:9yKSN-GCB0IC,cscdr: um classificador baseado em seleção clonal com redução de células de memória,"O sistema imunológico dos vertebrados é extremamente complexo, sendo responsável por proteger o organismo contra agentes causadores de doenças. Para funcionar apropriadamente, é necessário que seus componentes reconheçam de forma eficaz os elementos patógenos, a fim de neutralizá-los, e também os elementos do próprio organismo, de forma a não reagirem a estes. Estas e outras características são similares àquelas exigidas em soluções para problemas de engenharia e computação. Desta forma, os sistemas imunológicos artificiais utilizam a contraparte biológica como metáfora para o desenvolvimento de diversas ferramentas computacionais utilizadas nas mais diversas tarefas. Esta dissertação utiliza os conceitos apresentados pelos sistemas imunológicos artificiais para o desenvolvimento de um novo algoritmo de aprendizado supervisionado, baseado principalmente no mecanismo de seleção clonal. O método proposto neste trabalho, denominado clonal selection classifier with data reduction (CSCDR), utiliza uma função de aptidão com base no número de classificações corretas e incorretas apresentadas por cada anticorpo. O algoritmo tenta maximizar este valor através do processo de seleção clonal, envolvendo mutação, maturação de afinidade e seleção dos melhores indivíduos, transformando a fase de treinamento em um problema de otimização. Isto leva a anticorpos com maior representatividade e, portanto, diminui a quantidade de protótipos gerados ao final do algoritmo. Experimentos em bases de dados sintéticas e bases de dados de problemas reais, utilizadas como benchmark para problemas de …",2012/8,['ai_keywords'],[],['classifier'],,,,,
34,/citations?view_op=view_citation&hl=en&user=c6bK3vUAAAAJ&sortby=pubdate&citation_for_view=c6bK3vUAAAAJ:J_g5lzvAfSwC,a dispersion operator for geometric semantic genetic programming,"Recent advances in geometric semantic genetic programming (GSGP) have shown that the results obtained by these methods can outperform those obtained by classical genetic programming algorithms, in particular in the context of symbolic regression. However, there are still many open issues on how to improve their search mechanism. One of these issues is how to get around the fact that the GSGP crossover operator cannot generate solutions that are placed outside the convex hull formed by the individuals of the current population. Although the mutation operator alleviates this problem, we cannot guarantee it will find promising regions of the search space within feasible computational time. In this direction, this paper proposes a new geometric dispersion operator that uses multiplicative factors to move individuals to less dense areas of the search space around the target solution before applying semantic …",2016/7/20,['ai_keywords'],[],['regression'],,,,,
35,/citations?view_op=view_citation&hl=en&user=c6bK3vUAAAAJ&sortby=pubdate&citation_for_view=c6bK3vUAAAAJ:NaGl4SEjCO4C,reducing dimensionality to improve search in semantic genetic programming," Genetic programming approaches are moving from analysing the syntax of individual solutions to look into their semantics. One of the common definitions of the semantic space in the context of symbolic regression is a n-dimensional space, where n corresponds to the number of training examples. In problems where this number is high, the search process can became harder as the number of dimensions increase. Geometric semantic genetic programming (GSGP) explores the semantic space by performing geometric semantic operations—the fitness landscape seen by GSGP is guaranteed to be conic by construction. Intuitively, a lower number of dimensions can make search more feasible in this scenario, decreasing the chances of data overfitting and reducing the number of evaluations required to find a suitable solution. This paper proposes two approaches for dimensionality reduction in GSGP: (i) to …",2016,['ai_keywords'],[],['regression'],,,,,
36,/citations?view_op=view_citation&hl=en&user=c6bK3vUAAAAJ&sortby=pubdate&citation_for_view=c6bK3vUAAAAJ:O3NaXMp0MMsC,the effect of distinct geometric semantic crossover operators in regression problems," This paper investigates the impact of geometric semantic crossover operators in a wide range of symbolic regression problems. First, it analyses the impact of using Manhattan and Euclidean distance geometric semantic crossovers in the learning process. Then, it proposes two strategies to numerically optimize the crossover mask based on mathematical properties of these operators, instead of simply generating them randomly. An experimental analysis comparing geometric semantic crossovers using Euclidean and Manhattan distances and the proposed strategies is performed in a test bed of twenty datasets. The results show that the use of different distance functions in the semantic geometric crossover has little impact on the test error, and that our optimized crossover masks yield slightly better results. For SGP practitioners, we suggest the use of the semantic crossover based on the Euclidean …",2015,['ai_keywords'],[],['regression'],,,,,
37,/citations?view_op=view_citation&hl=en&user=c6bK3vUAAAAJ&sortby=pubdate&citation_for_view=c6bK3vUAAAAJ:RYcK_YlVTxYC,a generic framework for building dispersion operators in the semantic space," This chapter proposes a generic framework to build geometric dispersion (GD) operators for Geometric Semantic Genetic Programming in the context of symbolic regression, followed by two concrete instantiations of the framework: a multiplicative geometric dispersion operator and an additive geometric dispersion operator. These operators move individuals in the semantic space in order to balance the population around the target output in each dimension, with the objective of expanding the convex hull defined by the population to include the desired output vector. An experimental analysis was conducted in a testbed composed of sixteen datasets showing that dispersion operators can improve GSGP search and that the multiplicative version of the operator is overall better than the additive version.",2018,['ai_keywords'],[],['regression'],,,,,
38,/citations?view_op=view_citation&hl=en&user=c6bK3vUAAAAJ&sortby=pubdate&citation_for_view=c6bK3vUAAAAJ:SeFeTyx0c_EC,analysing symbolic regression benchmarks under a meta-learning approach,"The definition of a concise and effective testbed for Genetic Programming (GP) is a recurrent matter in the research community. This paper takes a new step in this direction, proposing a different approach to measure the quality of the symbolic regression benchmarks quantitatively. The proposed approach is based on meta-learning and uses a set of dataset meta-features---such as the number of examples or output skewness---to describe the datasets. Our idea is to correlate these meta-features with the errors obtained by a GP method. These meta-features define a space of benchmarks that should, ideally have datasets (points) covering different regions of the space. An initial analysis of 63 datasets showed that current benchmarks are concentrated in a small region of this benchmark space. We also found out that number of instances and output skewness are the most relevant meta-features to GP output error …",2018/7/6,['ai_keywords'],[],"['meta learning', 'regression']",,,,,
39,/citations?view_op=view_citation&hl=en&user=c6bK3vUAAAAJ&sortby=pubdate&citation_for_view=c6bK3vUAAAAJ:dfsIfKJdRG4C,instance selection for geometric semantic genetic programming,"Geometric Semantic Genetic Programming (GSGP) is a method that exploits the geometric properties describing the spatial relationship between possible solutions to a problem in an n-dimensional semantic space. In symbolic regression problems, n is equal to the number of training instances. Although very effective, the GSGP semantic space can become excessively big in most real applications, where the value of n is high, having a negative impact on the effectiveness of the GSGP search process. This paper tackles this problem by reducing the dimensionality of GSGP semantic space in symbolic regression problems using instance selection methods. Our approach relies on weighting functions-to estimate the relative importance of each instance based on its position with respect to its nearest neighbours-and on dimensionality reduction techniques-to improve the notion of closeness between instances …",2020/7/19,['ai_keywords'],[],"['nearest neighbour', 'regression']",,,,,
40,/citations?view_op=view_citation&hl=en&user=c6bK3vUAAAAJ&sortby=pubdate&citation_for_view=c6bK3vUAAAAJ:fPk4N6BV_jEC,improving search in geometric semantic genetic programming,"Making Genetic Programming methods semantic-aware has been the focus of many works in the past years. Among these methods, Geometric Semantic GP (GSGP) acts on the syntax of the parent programs producing offspring respecting a semantic criterion. In this thesis we focus on the open issues of GSGP. We investigate the impact of the geometric semantic crossover with different distance functions and the possibility of optimally adjusting its coefficients. We also present the Sequential Symbolic Regression, an attempt to control the exponential growth of the individuals caused by the use of this operator. In addition, we propose a Geometric Dispersion framework to construct operators that move individuals to less dense areas of the search space. Last, we present a study of the impact of selecting training instances in order to reduce the semantic space dimensionality. All methods proposed showed GSGP search can be improved by adding simple and effective mechanisms to its current operators",2016/9/28,['ai_keywords'],[],['regression'],,,,,
41,/citations?view_op=view_citation&hl=en&user=c6bK3vUAAAAJ&sortby=pubdate&citation_for_view=c6bK3vUAAAAJ:lSLTfruPkqcC,revisiting the sequential symbolic regression genetic programming,"Sequential Symbolic Regression (SSR) is a technique that recursively induces functions over the error of the current solution, concatenating them in an attempt to reduce the error of the resulting model. As proof of concept, the method was previously evaluated in one-dimensional problems and compared with canonical Genetic Programming (GP) and Geometric Semantic Genetic Programming (GSGP). In this paper we revisit SSR exploring the method behaviour in higher dimensional, larger and more heterogeneous datasets. We discuss the difficulties arising from the application of the method to more complex problems, e.g., overfitting, along with suggestions to overcome them. An experimental analysis was conducted comparing SSR to GP and GSGP, showing SSR solutions are smaller than those generated by the GSGP with similar performance and more accurate than those generated by the canonical GP.",2016/10/9,['ai_keywords'],[],['regression'],,,,,
42,/citations?view_op=view_citation&hl=en&user=cOeychwAAAAJ&sortby=pubdate&citation_for_view=cOeychwAAAAJ:u-x6o8ySG0sC,robust multiplexed model predictive control for agent-based conflict resolution,"Multiplexed Model Predictive Control (MMPC) was originally developed for a multi-input system as a strategy to reduce online computation. In this paper, we demonstrate how distributed control of a system of agents can be posed as a Multiplexed Model Predictive Control problem. Specifically, we consider using robust multiplexed MPC for controlling a system of agents in the presence of coupling constraints in the form of a collision avoidance requirement. The system is subject to persistent unknown, but bounded disturbances. The contribution of this paper is the extension of the original robust multiplexed MPC algorithm to include a disturbance feedback policy in between updates. This facilitates finding feasible solutions and inherits the property of rapid response to disturbances from multiplexing the control. In addition, it is observed that the computational time of the proposed MMPC scheme scales favourably …",2010/12/15,['ai_keywords'],[],['agent based'],,,,,
43,/citations?view_op=view_citation&hl=en&user=h0CniXgAAAAJ&sortby=pubdate&citation_for_view=h0CniXgAAAAJ:HbR8gkJAVGIC,learning robot control using a hierarchical som-based encoding,"Hierarchical representations and modeling of sensorimotor observations is a fundamental approach for the development of scalable robot control strategies. Previously, we introduced the novel Hierarchical Self-Organizing Map-based Encoding algorithm (HSOME) that is based on a computational model of infant cognition. Each layer is a temporally augmented SOM and every node updates a decaying activation value. The bottom level encodes sensori-motor instances while their temporal associations are hierarchically built on the layers above. In the past, HSOME has shown to support hierarchical encoding of sequential sensor-actuator observations both in abstract domains and real humanoid robots. Two novel features are presented here starting with the novel skill acquisition in the complex domain of learning a double tap tactile gesture between two humanoid robots. During reproduction, the robot can either perform a double tap or prioritize to receive a higher reward by performing a single tap instead. Secondly, HSOME has been extended to recall past observations and reproduce rhythmic patterns in the absence of input relevant to the joints by priming initially the reproduction of specific skills with an input. We also demonstrate in simulation how a complex behavior emerges from the automatic reuse of distinct oscillatory swimming demonstrations of a robotic salamander.",2017/1/17,['ai_keywords'],[],"['robotic', ' robot ']",,,,,
44,/citations?view_op=view_citation&hl=en&user=hAf_WPQAAAAJ&sortby=pubdate&citation_for_view=hAf_WPQAAAAJ:9yKSN-GCB0IC,data-driven extract method recommendations: an initial study at ing,"Refactoring is the process of improving the structure of code without changing its functionality. The process is beneficial for software quality but challenges remain for identifying refactoring opportunities. This work employs machine learning to predict the application of the refactoring type Extract Method in an industry setting with the use of code quality metrics. We detect 919 examples in industry code of Extract Method and 986 examples where Extract Method was not applied and compare this to open-source code. We find that feature distributions between industry and open-source code differ, especially in class-level metrics. We train models to predict Extract Method in industry code and find that Random Forests perform best. We find that class-level metrics are most important for the performance of these models. We then investigate whether models trained on an open-source set generalize to an industry setting. We find that, although less performant than a custom fit model, a Logistic Regression type model performs admirably. Afterward, we examine whether these models perform on unseen industry projects by validating on projects excluded from the training set. We find that average performance is decent but lower than when using the whole industry dataset or an open-source dataset for training. Lastly, we conduct a blind user study in which we ask experts to judge predictions made by our best model. We find that experts generally agree with the model's predictions. In the case that experts agree with the model's prediction to apply Extract Method, they do so because of high code complexity. When they agree with the model's prediction not …",2021,['ai_keywords'],[],"['machine learning', 'data driven', 'random forest', 'regression']",,,,,
45,/citations?view_op=view_citation&hl=en&user=hBZz3DUAAAAJ&sortby=pubdate&citation_for_view=hBZz3DUAAAAJ:RYcK_YlVTxYC,semantic topic compass–classification based on unsupervised feature ambiguity gradation," Characterising social media topics often requires new features to be continuously taken into account, and thus increasing the need for classifier retraining. One challenging aspect is the emergence of ambiguous features, which can affect classification performance. In this paper we investigate the impact of the use of ambiguous features in a topic classification task, and introduce the Semantic Topic Compass (STC) framework, which characterises ambiguity in a topics feature space. STC makes use of topic priors derived from structured knowledge sources to facilitate the semantic feature grading of a topic. Our findings demonstrate the proposed framework offers competitive boosts in performance across all datasets.",2016/5/29,['ai_keywords'],[],['classifier'],,,,,
46,/citations?view_op=view_citation&hl=en&user=hBZz3DUAAAAJ&sortby=pubdate&citation_for_view=hBZz3DUAAAAJ:RYcK_YlVTxYC,semantic topic compass–classification based on unsupervised feature ambiguity gradation," Characterising social media topics often requires new features to be continuously taken into account, and thus increasing the need for classifier retraining. One challenging aspect is the emergence of ambiguous features, which can affect classification performance. In this paper we investigate the impact of the use of ambiguous features in a topic classification task, and introduce the Semantic Topic Compass (STC) framework, which characterises ambiguity in a topics feature space. STC makes use of topic priors derived from structured knowledge sources to facilitate the semantic feature grading of a topic. Our findings demonstrate the proposed framework offers competitive boosts in performance across all datasets.",2016/5/29,['ai_keywords'],[],['classifier'],,,,,
47,/citations?view_op=view_citation&hl=en&user=hBZz3DUAAAAJ&sortby=pubdate&citation_for_view=hBZz3DUAAAAJ:ldfaerwXgEUC,a semantic graph-based approach for radicalisation detection on social media," From its start, the so-called Islamic State of Iraq and the Levant (ISIL/ISIS) has been successfully exploiting social media networks, most notoriously Twitter, to promote its propaganda and recruit new members, resulting in thousands of social media users adopting a pro-ISIS stance every year. Automatic identification of pro-ISIS users on social media has, thus, become the centre of interest for various governmental and research organisations. In this paper we propose a semantic graph-based approach for radicalisation detection on Twitter. Unlike previous works, which mainly rely on the lexical representation of the content published by Twitter users, our approach extracts and makes use of the underlying semantics of words exhibited by these users to identify their pro/anti-ISIS stances. Our results show that classifiers trained from semantic features outperform those trained from lexical, sentiment, topic and …",2017/5/28,['ai_keywords'],[],['classifier'],,,,,
48,/citations?view_op=view_citation&hl=en&user=hBZz3DUAAAAJ&sortby=pubdate&citation_for_view=hBZz3DUAAAAJ:ldfaerwXgEUC,a semantic graph-based approach for radicalisation detection on social media," From its start, the so-called Islamic State of Iraq and the Levant (ISIL/ISIS) has been successfully exploiting social media networks, most notoriously Twitter, to promote its propaganda and recruit new members, resulting in thousands of social media users adopting a pro-ISIS stance every year. Automatic identification of pro-ISIS users on social media has, thus, become the centre of interest for various governmental and research organisations. In this paper we propose a semantic graph-based approach for radicalisation detection on Twitter. Unlike previous works, which mainly rely on the lexical representation of the content published by Twitter users, our approach extracts and makes use of the underlying semantics of words exhibited by these users to identify their pro/anti-ISIS stances. Our results show that classifiers trained from semantic features outperform those trained from lexical, sentiment, topic and …",2017/5/28,['ai_keywords'],[],['classifier'],,,,,
49,/citations?view_op=view_citation&hl=en&user=hBZz3DUAAAAJ&sortby=pubdate&citation_for_view=hBZz3DUAAAAJ:ldfaerwXgEUC,a semantic graph-based approach for radicalisation detection on social media," From its start, the so-called Islamic State of Iraq and the Levant (ISIL/ISIS) has been successfully exploiting social media networks, most notoriously Twitter, to promote its propaganda and recruit new members, resulting in thousands of social media users adopting a pro-ISIS stance every year. Automatic identification of pro-ISIS users on social media has, thus, become the centre of interest for various governmental and research organisations. In this paper we propose a semantic graph-based approach for radicalisation detection on Twitter. Unlike previous works, which mainly rely on the lexical representation of the content published by Twitter users, our approach extracts and makes use of the underlying semantics of words exhibited by these users to identify their pro/anti-ISIS stances. Our results show that classifiers trained from semantic features outperform those trained from lexical, sentiment, topic and …",2017/5/28,['ai_keywords'],[],['classifier'],,,,,
50,/citations?view_op=view_citation&hl=en&user=hBZz3DUAAAAJ&sortby=pubdate&citation_for_view=hBZz3DUAAAAJ:vV6vV6tmYwMC,on the role of semantics for detecting pro-isis stances on social media,"From its start, the so-called Islamic State of Iraq and the Levant (ISIL/ISIS) has been successfully exploiting social media networks, most notoriously Twitter, to promote its propaganda and recruit new members, resulting in thousands of social media users adopting pro ISIS stance every year. Automatic identification of pro-ISIS users on social media has, thus, become the centre of interest for various governmental and research organisations. In this paper we propose a semantic-based approach for radicalisation detection on Twitter. Unlike most previous works, which mainly rely on the lexical and contextual representation of the content published by Twitter users, our approach extracts and makes use of the underlying semantics of words exhibited by these users to identify their pro/anti-ISIS stances. Our results show that classifiers trained from words’ semantics outperform those trained from lexical and network features by 2% on average F1-measure.",2016/10,['ai_keywords'],[],['classifier'],,,,,
51,/citations?view_op=view_citation&hl=en&user=hBZz3DUAAAAJ&sortby=pubdate&citation_for_view=hBZz3DUAAAAJ:vV6vV6tmYwMC,on the role of semantics for detecting pro-isis stances on social media,"From its start, the so-called Islamic State of Iraq and the Levant (ISIL/ISIS) has been successfully exploiting social media networks, most notoriously Twitter, to promote its propaganda and recruit new members, resulting in thousands of social media users adopting pro ISIS stance every year. Automatic identification of pro-ISIS users on social media has, thus, become the centre of interest for various governmental and research organisations. In this paper we propose a semantic-based approach for radicalisation detection on Twitter. Unlike most previous works, which mainly rely on the lexical and contextual representation of the content published by Twitter users, our approach extracts and makes use of the underlying semantics of words exhibited by these users to identify their pro/anti-ISIS stances. Our results show that classifiers trained from words’ semantics outperform those trained from lexical and network features by 2% on average F1-measure.",2016/10,['ai_keywords'],[],['classifier'],,,,,
52,/citations?view_op=view_citation&hl=en&user=jrsxlTwAAAAJ&sortby=pubdate&citation_for_view=jrsxlTwAAAAJ:2osOgNQ5qMEC,multi-event crisis management using non-cooperative repeated games,"The optimal allocation of the resources to the emergency locations in the event of multiple crises in an urban environment is an intricate problem, especially when the available resources are limited. In such a scenario, it is important to allocate emergency response units in a fair manner based on the criticality of the crisis events and their requests. In this research, a crisis management tool is developed which incorporates a resource allocation algorithm. The problem is formulated as a game theoretic framework in which the crisis events are modeled as the players, the emergency response centers as the resource locations with emergency units to be scheduled and the possible allocations as strategies. The pay-off is modeled as a function of the criticality of the event and the anticipated response times. The game is played assuming a specific region within a certain locality of the crisis event to derive an optimal allocation. If a solution is not feasible, the perimeter of the locality in consideration is increased and the game is repeated until convergence. Experimental results are presented to illustrate the efficacy of the proposed methodology and metrics are derived to quantify the fairness of the solution. A regression analysis has been performed to identify the statistical significance of the results.",2004/11/19,[],[],[],,,,,
53,/citations?view_op=view_citation&hl=en&user=jrsxlTwAAAAJ&sortby=pubdate&citation_for_view=jrsxlTwAAAAJ:2osOgNQ5qMEC,multi-event crisis management using non-cooperative repeated games,"The optimal allocation of the resources to the emergency locations in the event of multiple crises in an urban environment is an intricate problem, especially when the available resources are limited. In such a scenario, it is important to allocate emergency response units in a fair manner based on the criticality of the crisis events and their requests. In this research, a crisis management tool is developed which incorporates a resource allocation algorithm. The problem is formulated as a game theoretic framework in which the crisis events are modeled as the players, the emergency response centers as the resource locations with emergency units to be scheduled and the possible allocations as strategies. The pay-off is modeled as a function of the criticality of the event and the anticipated response times. The game is played assuming a specific region within a certain locality of the crisis event to derive an optimal allocation. If a solution is not feasible, the perimeter of the locality in consideration is increased and the game is repeated until convergence. Experimental results are presented to illustrate the efficacy of the proposed methodology and metrics are derived to quantify the fairness of the solution. A regression analysis has been performed to identify the statistical significance of the results.",2004/11/19,['ai_keywords'],[],['regression'],,,,,
54,/citations?view_op=view_citation&hl=en&user=kOMk-s4AAAAJ&sortby=pubdate&citation_for_view=kOMk-s4AAAAJ:M3NEmzRMIkIC,a clustering-based reinforcement learning approach for tailored personalization of e-health interventions,"Personalization is very powerful in improving the effectiveness of health interventions. Reinforcement learning (RL) algorithms are suitable for learning these tailored interventions from sequential data collected about individuals. However, learning can be very fragile. The time to learn intervention policies is limited as disengagement from the user can occur quickly. Also, in e-Health intervention timing can be crucial before the optimal window passes. We present an approach that learns tailored personalization policies for groups of users by combining RL and clustering. The benefits are two-fold: speeding up the learning to prevent disengagement while maintaining a high level of personalization. Our clustering approach utilizes dynamic time warping to compare user trajectories consisting of states and rewards. We apply online and batch RL to learn policies over clusters of individuals and introduce our self …",2018/4,['ai_keywords'],[],"['reinforcement learning', 'clustering']",,,,,
55,/citations?view_op=view_citation&hl=en&user=kgh4wYMAAAAJ&sortby=pubdate&citation_for_view=kgh4wYMAAAAJ:LlXTz_FrCmAC,measurement of the photon identification efficiencies with the atlas detector using lhc run-1 data," The algorithms used by the ATLAS Collaboration to reconstruct and identify prompt photons are described. Measurements of the photon identification efficiencies are reported, using 4.9 fb of pp collision data collected at the LHC at   and 20.3 fb at  . The efficiencies are measured separately for converted and unconverted photons, in four different pseudorapidity regions, for transverse momenta between 10  and 1.5 . The results from the combination of three data-driven techniques are compared to the predictions from a simulation of the detector response, after correcting the electromagnetic shower momenta in the simulation for the average differences observed with respect to data. Data-to-simulation efficiency ratios used as correction factors in physics measurements are determined to account for the small residual efficiency differences. These factors are measured with …",2016/12/3,[],[],[],,,,,
56,/citations?view_op=view_citation&hl=en&user=kgh4wYMAAAAJ&sortby=pubdate&citation_for_view=kgh4wYMAAAAJ:LlXTz_FrCmAC,measurement of the photon identification efficiencies with the atlas detector using lhc run-1 data," The algorithms used by the ATLAS Collaboration to reconstruct and identify prompt photons are described. Measurements of the photon identification efficiencies are reported, using 4.9 fb of pp collision data collected at the LHC at   and 20.3 fb at  . The efficiencies are measured separately for converted and unconverted photons, in four different pseudorapidity regions, for transverse momenta between 10  and 1.5 . The results from the combination of three data-driven techniques are compared to the predictions from a simulation of the detector response, after correcting the electromagnetic shower momenta in the simulation for the average differences observed with respect to data. Data-to-simulation efficiency ratios used as correction factors in physics measurements are determined to account for the small residual efficiency differences. These factors are measured with …",2016/12,['ai_keywords'],[],['data driven'],,,,,
57,/citations?view_op=view_citation&hl=en&user=kgh4wYMAAAAJ&sortby=pubdate&citation_for_view=kgh4wYMAAAAJ:gnsKu8c89wgC,top-quark mass measurement in the all-hadronic  decay channel at  tev with the atlas detector,The top-quark mass is measured in the all-hadronic top-antitop quark decay channel using proton-proton collisions at a centre-of-mass energy of TeV with the ATLAS detector at the CERN Large Hadron Collider. The data set used in the analysis corresponds to an integrated luminosity of 20. 2 fb− 1. The large multi-jet background is modelled using a data-driven method. The top-quark mass is obtained from template fits to the ratio of the three-jet to the dijet mass. The three-jet mass is obtained from the three jets assigned to the top quark decay. From these three jets the dijet mass is obtained using the two jets assigned to the W boson decay. The top-quark mass is measured to be 173. 72±0. 55 (stat.)±1. 01 (syst.) GeV.,2017/9,['ai_keywords'],[],['data driven'],,,,,
58,/citations?view_op=view_citation&hl=en&user=kgh4wYMAAAAJ&sortby=pubdate&citation_for_view=kgh4wYMAAAAJ:gnsKu8c89wgC,top-quark mass measurement in the all-hadronic  decay channel at  tev with the atlas detector,The top-quark mass is measured in the all-hadronic top-antitop quark decay channel using proton-proton collisions at a centre-of-mass energy of TeV with the ATLAS detector at the CERN Large Hadron Collider. The data set used in the analysis corresponds to an integrated luminosity of 20. 2 fb− 1. The large multi-jet background is modelled using a data-driven method. The top-quark mass is obtained from template fits to the ratio of the three-jet to the dijet mass. The three-jet mass is obtained from the three jets assigned to the top quark decay. From these three jets the dijet mass is obtained using the two jets assigned to the W boson decay. The top-quark mass is measured to be 173. 72±0. 55 (stat.)±1. 01 (syst.) GeV.,2017/9,['ai_keywords'],[],['data driven'],,,,,
59,/citations?view_op=view_citation&hl=en&user=kgh4wYMAAAAJ&sortby=pubdate&citation_for_view=kgh4wYMAAAAJ:wyCGhLAOp5UC,measurement of the cross-section for producing a  boson in association with a single top quark in  collisions at  tev with atlas,"The inclusive cross-section for the associated production of a W boson and top quark is measured using data from proton-proton collisions at TeV. The dataset corresponds to an integrated luminosity of 3.2 fb− 1, and was collected in 2015 by the ATLAS detector at the Large Hadron Collider at CERN. Events are selected requiring two opposite sign isolated leptons and at least one jet; they are separated into signal and control regions based on their jet multiplicity and the number of jets that are identified as containing b hadrons. The W t signal is then separated from the background using boosted decision tree discriminants in two regions. The cross-section is extracted by fitting templates to the data distributions, and is measured to be σ W t= 94±10 (stat.)+ 28− 22 (syst.)±2 (lumi.) pb. The measured value is in good agreement with the SM prediction of σ theory= 71. 7±1. 8 (scale)±3. 4 (PDF) pb [1].",2018/1,['ai_keywords'],[],['decision tree'],,,,,
60,/citations?view_op=view_citation&hl=en&user=kgh4wYMAAAAJ&sortby=pubdate&citation_for_view=kgh4wYMAAAAJ:wyCGhLAOp5UC,measurement of the cross-section for producing a  boson in association with a single top quark in  collisions at  tev with atlas,"The inclusive cross-section for the associated production of a W boson and top quark is measured using data from proton-proton collisions at TeV. The dataset corresponds to an integrated luminosity of 3.2 fb− 1, and was collected in 2015 by the ATLAS detector at the Large Hadron Collider at CERN. Events are selected requiring two opposite sign isolated leptons and at least one jet; they are separated into signal and control regions based on their jet multiplicity and the number of jets that are identified as containing b hadrons. The W t signal is then separated from the background using boosted decision tree discriminants in two regions. The cross-section is extracted by fitting templates to the data distributions, and is measured to be σ W t= 94±10 (stat.)+ 28− 22 (syst.)±2 (lumi.) pb. The measured value is in good agreement with the SM prediction of σ theory= 71. 7±1. 8 (scale)±3. 4 (PDF) pb [1].",2018/1,['ai_keywords'],[],['decision tree'],,,,,
61,/citations?view_op=view_citation&hl=en&user=kgh4wYMAAAAJ&sortby=pubdate&citation_for_view=kgh4wYMAAAAJ:wyCGhLAOp5UC,measurement of the cross-section for producing a  boson in association with a single top quark in  collisions at  tev with atlas,"The inclusive cross-section for the associated production of a W boson and top quark is measured using data from proton-proton collisions at TeV. The dataset corresponds to an integrated luminosity of 3.2 fb− 1, and was collected in 2015 by the ATLAS detector at the Large Hadron Collider at CERN. Events are selected requiring two opposite sign isolated leptons and at least one jet; they are separated into signal and control regions based on their jet multiplicity and the number of jets that are identified as containing b hadrons. The W t signal is then separated from the background using boosted decision tree discriminants in two regions. The cross-section is extracted by fitting templates to the data distributions, and is measured to be σ W t= 94±10 (stat.)+ 28− 22 (syst.)±2 (lumi.) pb. The measured value is in good agreement with the SM prediction of σ theory= 71. 7±1. 8 (scale)±3. 4 (PDF) pb [1].",2018/1,['ai_keywords'],[],['decision tree'],,,,,
62,/citations?view_op=view_citation&hl=en&user=nLBUZZYAAAAJ&sortby=pubdate&citation_for_view=nLBUZZYAAAAJ:3fE2CSJIrl8C,exploring tangible algorithmic imaginaries in movie recommendations,"Recommender algorithms play an active role in many everyday activities. However, personalized recommendations often produce negative experiences due to a lack of awareness, control, or transparency. Allowing users to materialize their algorithmic imaginaries exposes how they experience, perceive, and imagine recommender algorithms. Moreover, it can unearth novel and previously unattended design opportunities for tangible interactions with algorithms. Therefore, we explored how 15 users of a famous movie recommender system materialized tangible designs to reflect and discuss their algorithmic imaginaries during co-design workshops and interviews. Using thematic analysis, we identified two forms of algorithmic imaginaries that can inspire tangible interactions with recommender algorithms: metaphoric and datafied representations. Complementary themes exposed the influence of contextual factors and diverse negative attitudes towards personalized movie recommendations. Based on these findings, we suggest design opportunities and suggestions for improving the algorithmic experience of movie recommendations and similar systems through tangible user interfaces.",2021/2/17,['ai_keywords'],[],['recommender'],,,,,
63,/citations?view_op=view_citation&hl=en&user=nLBUZZYAAAAJ&sortby=pubdate&citation_for_view=nLBUZZYAAAAJ:IWHjjKOFINEC,phara: a personal health augmented reality assistant to support decision-making at grocery stores,"Poor diet and physical inactivity are important factors that con-tribute to the obesity outbreak. erefore, healthy eating habits are crucial for physical well-being. In this paper, we present the concept design and an early stage evaluation of PHARA, a personal health augmented reality assistant that recommends healthy and similar products to people in their everyday lives. We evaluated a content-based recommender system in a desktop environment (n= 15) to measure the perceived quality, as well as behavioral intentions of users. In addition, we evaluated the user interface and measured participants’ perceptions of usefulness and ease of use. Whereas perceived usefulness and perceived ease of use are good, more work is required towards improving the accuracy and diversity of recommendations.",2017/8/12,['ai_keywords'],[],['recommender'],,,,,
64,/citations?view_op=view_citation&hl=en&user=nLBUZZYAAAAJ&sortby=pubdate&citation_for_view=nLBUZZYAAAAJ:R3hNpaxXUhUC,managing uncertainty in visual analytics: designing explanation interfaces for recommender systems and prediction models,"Visual Analytics is the science that studies the visual representation of information to enable interactive analysis of large and/or complex sets of information. These representations provide a guide to the thinking process of the decision-maker to facilitate correct interpretation of otherwise intractable data. The nature of the data, however, can heavily affect the effectiveness of guidance. Missing measurements, heterogeneous sources and errors in the data are usually present in even the most curated data. In addition, limitations of the analysis algorithm can introduce a bias. Current visualization techniques proposed by Visual Analytics fail to represent this uncertainty. The lack of awareness about data quality could lead the decision-maker to take a course of action based on flawed information. Our project will research new visualization techniques that can communicate the underlying uncertainty as estimated by data-processing experts to domain-experts that have little or no knowledge on data-processing. These visualization techniques will help domain-experts to establish their degree of trust in the presented information and to take decisions with a greater knowledge of the quality of the data. In addition, the project will research interaction techniques to capture feedback from domain-experts and to incorporate such feedback in the analysis process.",2019,['ai_keywords'],[],['recommender'],,,,,
65,/citations?view_op=view_citation&hl=en&user=nLBUZZYAAAAJ&sortby=pubdate&citation_for_view=nLBUZZYAAAAJ:TQgYirikUcIC,estudios de eliminación de microorganismos patógenos de residuales porcinos en un biorreactor con tiempo de retención corto,"Se estudiaron variables de operación de un biorreactor tubular en condiciones experimentales (dimensiones, 68.58 cm de largo y 15.24 cm de diámetro), y se utilizó un diseño experimental completamente aleatorio, con un arreglo factorial 2x2x2 con tres réplicas por tratamiento. Los factores fueron el aislamiento térmico del reactor (aislado o no)., la velocidad de agitación (0 y 0.6 rpm) y la hermeticidad (abierto o cerrado). Para un estudio de la cinética, se realizaron mediciones de pH y temperatura a los siguientes tiempos: 0, 10, 12, 14, 16, 18, 20 y 23 hr. Los residuales porcinos se mezclaron con rastrojo de sorgo, melaza, suero de leche y agua (MS promedio inicial, 21.2%).La desaparición de organismos patógenos (Salmonella spp, Shigella spp y Escherichia coli) tuvo lugar entre 16 y 23 hr en todos los tratamientos (conteo mínimo en el tiempo cero, 10-6 UFC/mL), y los mejores tratamientos fueron cuando la fermentación ocurrió en condiciones abiertas y con agitación, sin necesidad de aislamiento térmico. El tiempo mínimo para alcanzar un pH de 4.0 fue de 16 hr. La calidad nutricional de la mezcla con residuales porcinos también influenciaron las condiciones del proceso. El contenido promedio proteico (Nx6. 25) de la mezcla fermentada fue un 10% más que el valor inicial, al pasar de 11.7 a 15.0% en base seca (P< 0.05). No hubo cambios en la concentración final de MS (21.7%).",2004,['ai_keywords'],[],[' ml '],,,,,
66,/citations?view_op=view_citation&hl=en&user=pOcS2dkAAAAJ&sortby=pubdate&citation_for_view=pOcS2dkAAAAJ:hC7cP41nSMkC,federated sinkhorn,"In this work we investigate the potential of solving the discrete Optimal Transport (OT) problem with entropy regularization in a federated learning setting. Recall that the celebrated Sinkhorn algorithm transforms the classical OT linear program into strongly convex constrained optimization, facilitating first order methods for otherwise intractably large problems. A common contemporary setting that remains an open problem as far as the application of Sinkhorn is the presence of data spread across clients with distributed inter-communication, either due to clients whose privacy is a concern, or simply by necessity of processing and memory hardware limitations. In this work we investigate various natural procedures, which we refer to as Federated Sinkhorn, that handle distributed environments where data is partitioned across multiple clients. We formulate the problem as minimizing the transport cost with an entropy regularization term, subject to marginal constraints, where block components of the source and target distribution vectors are locally known to clients corresponding to each block. We consider both synchronous and asynchronous variants as well as all-to-all and server-client communication topology protocols. Each procedure allows clients to compute local operations on their data partition while periodically exchanging information with others. We provide theoretical guarantees on convergence for the different variants under different possible conditions. We empirically demonstrate the algorithms performance on synthetic datasets and a real-world financial risk assessment application. The investigation highlights the subtle tradeoffs …",2025/2/10,['ai_keywords'],[],"['synthetic data', 'federated learning']",,,,,
67,/citations?view_op=view_citation&hl=en&user=rRki_pAAAAAJ&sortby=pubdate&citation_for_view=rRki_pAAAAAJ:UeHWp8X0CEIC,laboratory calibration and performance evaluation of low-cost capacitive and very low-cost resistive soil moisture sensors,"Soil volumetric water content (VWC) is a vital parameter to understand several ecohydrological and environmental processes. Its cost-effective measurement can potentially drive various technological tools to promote data-driven sustainable agriculture through supplemental irrigation solutions, the lack of which has contributed to severe agricultural distress, particularly for smallholder farmers. The cost of commercially available VWC sensors varies over four orders of magnitude. A laboratory study characterizing and testing sensors from this wide range of cost categories, which is a prerequisite to explore their applicability for irrigation management, has not been conducted. Within this context, two low-cost capacitive sensors—SMEC300 and SM100—manufactured by Spectrum Technologies Inc. (Aurora, IL, USA), and two very low-cost resistive sensors—the Soil Hygrometer Detection Module Soil Moisture Sensor (YL100) by Electronicfans and the Generic Soil Moisture Sensor Module (YL69) by KitsGuru—were tested for performance in laboratory conditions. Each sensor was calibrated in different repacked soils, and tested to evaluate accuracy, precision and sensitivity to variations in temperature and salinity. The capacitive sensors were additionally tested for their performance in liquids of known dielectric constants, and a comparative analysis of the calibration equations developed in-house and provided by the manufacturer was carried out. The value for money of the sensors is reflected in their precision performance, i.e., the precision performance largely follows sensor costs. The other aspects of sensor performance do not necessarily follow …",2020/1/8,['ai_keywords'],[],['data driven'],,,,,
68,/citations?view_op=view_citation&hl=en&user=rRki_pAAAAAJ&sortby=pubdate&citation_for_view=rRki_pAAAAAJ:_kc_bZDykSQC,impact of calibrating a low-cost capacitance-based soil moisture sensor on aquacrop model performance,"Sensor data and agro-hydrological modeling have been combined to improve irrigation management. Crop water models simulating crop growth and production in response to the soil-water environment need to be parsimonious in terms of structure, inputs and parameters to be applied in data scarce regions. Irrigation management using soil moisture sensors requires them to be site-calibrated, low-cost, and maintainable. Therefore, there is a need for parsimonious crop modeling combined with low-cost soil moisture sensing without losing predictive capability.This study calibrated the low-cost capacitance-based Spectrum Inc. SM100 soil moisture sensor using multiple least squares and machine learning models, with both laboratory and field data. The best calibration technique, field-based piece-wise linear regression (calibration r2 = 0.76, RMSE = 3.13 %, validation r2 = 0.67, RMSE = 4.57 %), was used to study …",2024/2/27,['ai_keywords'],[],"['machine learning', 'regression']",,,,,
